{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques dates\n",
    "Avant le 21/04 :\n",
    "¤ Remise du rapport \n",
    "   - Entre 10 et 20 pages\n",
    "   - Pas de code dans le rapport\n",
    "   I: Introduction  (  (3))  (1-2pages)\n",
    "   - Court\n",
    "   - Résumer la tache effectuer par le RN (spécificités, votre approche, le plan)\n",
    "   - Présenter l'état de l'art (ce qui a déja été fait) et si on s'en est inspiré\n",
    "   II: Les données ( ensuite decrire les données (2)) (2 pages max)\n",
    "   - Décrire les entrées/sorties ( nombre de taches abordées)\n",
    "   - Statistique:\n",
    "       - Taille des données train/test\n",
    "       - Vocabulaire\n",
    "       - Répartition classe\n",
    "       - Filtrage / selection des données\n",
    "   III: Le modéle ( Commencer par cette partie  (1) )   (5-6 pages)\n",
    "   - Définir le modèle ( faire une figure illustrer)\n",
    "   - Résultats/accuracy/courbe d'apprentissage (nombre d'époques, temps total, stratégie d'apprentissage utilisée)\n",
    "       -  \n",
    "   - Expliquer les coubres/diagrammes/tableaux\n",
    "   \n",
    "   \n",
    "   IV: Conclusion (1 page)\n",
    "   - Résumer de ce qui a été fait, et ce qui reste à faire, comment on pourrait le faire \n",
    "   \n",
    "   Exemple bAbi task: Monde clos vocabulaire générer et fixes\n",
    "   Important donner des exemples\n",
    "Mercredi 26/04,  Vendredi 28/04,   Mardi 02/05\n",
    "- Soutenance TER ( date encore à déterminer)\n",
    "  20 minutes exposé/questions\n",
    " \n",
    " \n",
    " \n",
    " A rendre Rapport + code -> archive\n",
    " Rapport: nom.pdf\n",
    " Archive: nom.tgz/zip/rar\n",
    " \n",
    " A envoyer à l'adresse:\n",
    " allauzen@limsi.fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN / Embed / Sent / Query = <class 'keras.layers.recurrent.LSTM'>, 50, 100, 100\n",
      "vocab = [u'.', u'?', u'Daniel', u'John', u'Mary', u'Sandra', u'Where', u'back', u'bathroom', u'bedroom', u'garden', u'hallway', u'is', u'journeyed', u'kitchen', u'moved', u'office', u'the', u'to', u'travelled', u'went']\n",
      "X.shape = (10000, 68)\n",
      "Xq.shape = (10000, 4)\n",
      "Y.shape = (10000, 22)\n",
      "story_maxlen, query_maxlen = 68, 4\n",
      "Build model...\n",
      "Training\n",
      "Train on 9500 samples, validate on 500 samples\n",
      "Epoch 1/40\n",
      "9500/9500 [==============================] - 56s - loss: 1.8661 - acc: 0.2416 - val_loss: 1.5454 - val_acc: 0.4420\n",
      "Epoch 2/40\n",
      "9500/9500 [==============================] - 56s - loss: 1.4270 - acc: 0.4731 - val_loss: 1.3776 - val_acc: 0.4940\n",
      "Epoch 3/40\n",
      "9500/9500 [==============================] - 56s - loss: 1.2909 - acc: 0.5101 - val_loss: 1.2513 - val_acc: 0.5240\n",
      "Epoch 4/40\n",
      "9500/9500 [==============================] - 56s - loss: 1.2167 - acc: 0.5131 - val_loss: 1.1961 - val_acc: 0.4900\n",
      "Epoch 5/40\n",
      "9500/9500 [==============================] - 56s - loss: 1.1692 - acc: 0.5147 - val_loss: 1.1816 - val_acc: 0.4940\n",
      "Epoch 6/40\n",
      "9500/9500 [==============================] - 56s - loss: 1.1475 - acc: 0.5137 - val_loss: 1.1688 - val_acc: 0.5040\n",
      "Epoch 7/40\n",
      "9500/9500 [==============================] - 55s - loss: 1.1280 - acc: 0.5128 - val_loss: 1.1519 - val_acc: 0.5040\n",
      "Epoch 8/40\n",
      "9500/9500 [==============================] - 55s - loss: 1.1131 - acc: 0.5186 - val_loss: 1.1412 - val_acc: 0.4880\n",
      "Epoch 9/40\n",
      "9500/9500 [==============================] - 55s - loss: 1.0984 - acc: 0.5241 - val_loss: 1.1485 - val_acc: 0.5040\n",
      "Epoch 10/40\n",
      "9500/9500 [==============================] - 55s - loss: 1.0924 - acc: 0.5203 - val_loss: 1.1385 - val_acc: 0.5100\n",
      "Epoch 11/40\n",
      "9500/9500 [==============================] - 56s - loss: 1.0841 - acc: 0.5217 - val_loss: 1.1154 - val_acc: 0.5120\n",
      "Epoch 12/40\n",
      "9500/9500 [==============================] - 55s - loss: 1.0760 - acc: 0.5234 - val_loss: 1.1334 - val_acc: 0.5020\n",
      "Epoch 13/40\n",
      "9500/9500 [==============================] - 56s - loss: 1.0743 - acc: 0.5187 - val_loss: 1.1206 - val_acc: 0.4960\n",
      "Epoch 14/40\n",
      "9500/9500 [==============================] - 56s - loss: 1.0618 - acc: 0.5228 - val_loss: 1.1137 - val_acc: 0.5000\n",
      "Epoch 15/40\n",
      "9500/9500 [==============================] - 56s - loss: 1.0540 - acc: 0.5272 - val_loss: 1.1157 - val_acc: 0.5020\n",
      "Epoch 16/40\n",
      "9500/9500 [==============================] - 56s - loss: 1.0600 - acc: 0.5175 - val_loss: 1.1147 - val_acc: 0.4900\n",
      "Epoch 17/40\n",
      "9500/9500 [==============================] - 56s - loss: 1.0452 - acc: 0.5294 - val_loss: 1.1261 - val_acc: 0.5060\n",
      "Epoch 18/40\n",
      "9500/9500 [==============================] - 56s - loss: 1.0487 - acc: 0.5252 - val_loss: 1.1034 - val_acc: 0.5160\n",
      "Epoch 19/40\n",
      "9500/9500 [==============================] - 55s - loss: 1.0407 - acc: 0.5301 - val_loss: 1.1212 - val_acc: 0.4980\n",
      "Epoch 20/40\n",
      "9500/9500 [==============================] - 56s - loss: 1.0371 - acc: 0.5267 - val_loss: 1.1134 - val_acc: 0.4920\n",
      "Epoch 21/40\n",
      "9500/9500 [==============================] - 56s - loss: 1.0280 - acc: 0.5288 - val_loss: 1.1040 - val_acc: 0.5100\n",
      "Epoch 22/40\n",
      "9500/9500 [==============================] - 56s - loss: 1.0288 - acc: 0.5304 - val_loss: 1.1053 - val_acc: 0.5140\n",
      "Epoch 23/40\n",
      "9500/9500 [==============================] - 55s - loss: 1.0193 - acc: 0.5334 - val_loss: 1.1134 - val_acc: 0.4820\n",
      "Epoch 24/40\n",
      "9500/9500 [==============================] - 55s - loss: 1.0138 - acc: 0.5399 - val_loss: 1.1155 - val_acc: 0.5140\n",
      "Epoch 25/40\n",
      "9500/9500 [==============================] - 56s - loss: 1.0117 - acc: 0.5371 - val_loss: 1.0993 - val_acc: 0.5160\n",
      "Epoch 26/40\n",
      "9500/9500 [==============================] - 56s - loss: 0.9994 - acc: 0.5496 - val_loss: 1.0903 - val_acc: 0.5280\n",
      "Epoch 27/40\n",
      "9500/9500 [==============================] - 56s - loss: 0.9829 - acc: 0.5568 - val_loss: 1.0682 - val_acc: 0.5340\n",
      "Epoch 28/40\n",
      "9500/9500 [==============================] - 56s - loss: 0.9468 - acc: 0.5818 - val_loss: 1.0199 - val_acc: 0.5520\n",
      "Epoch 29/40\n",
      "9500/9500 [==============================] - 56s - loss: 0.8874 - acc: 0.6249 - val_loss: 0.9548 - val_acc: 0.6140\n",
      "Epoch 30/40\n",
      "9500/9500 [==============================] - 55s - loss: 0.7960 - acc: 0.6762 - val_loss: 0.7782 - val_acc: 0.7100\n",
      "Epoch 31/40\n",
      "9500/9500 [==============================] - 56s - loss: 0.6260 - acc: 0.7678 - val_loss: 0.5217 - val_acc: 0.8180\n",
      "Epoch 32/40\n",
      "9500/9500 [==============================] - 56s - loss: 0.4227 - acc: 0.8581 - val_loss: 0.2716 - val_acc: 0.9060\n",
      "Epoch 33/40\n",
      "9500/9500 [==============================] - 56s - loss: 0.2319 - acc: 0.9338 - val_loss: 0.1528 - val_acc: 0.9620\n",
      "Epoch 34/40\n",
      "9500/9500 [==============================] - 56s - loss: 0.1355 - acc: 0.9676 - val_loss: 0.0639 - val_acc: 0.9880\n",
      "Epoch 35/40\n",
      "9500/9500 [==============================] - 56s - loss: 0.0764 - acc: 0.9825 - val_loss: 0.0355 - val_acc: 0.9940\n",
      "Epoch 36/40\n",
      "9500/9500 [==============================] - 56s - loss: 0.0532 - acc: 0.9898 - val_loss: 0.0277 - val_acc: 0.9940\n",
      "Epoch 37/40\n",
      "9500/9500 [==============================] - 56s - loss: 0.0447 - acc: 0.9927 - val_loss: 0.0089 - val_acc: 0.9980\n",
      "Epoch 38/40\n",
      "9500/9500 [==============================] - 56s - loss: 0.0269 - acc: 0.9958 - val_loss: 0.0093 - val_acc: 0.9980\n",
      "Epoch 39/40\n",
      "9500/9500 [==============================] - 56s - loss: 0.0147 - acc: 0.9987 - val_loss: 0.0069 - val_acc: 0.9980\n",
      "Epoch 40/40\n",
      "9500/9500 [==============================] - 56s - loss: 0.0147 - acc: 0.9978 - val_loss: 0.0164 - val_acc: 0.9960\n",
      "1000/1000 [==============================] - 1s     \n",
      "Test loss / test accuracy = 0.0265 / 0.9940\n",
      "acc: 99.40%\n"
     ]
    }
   ],
   "source": [
    "'''Trains two recurrent neural networks based upon a story and a question.\n",
    "The resulting merged vector is then queried to answer a range of bAbI tasks.\n",
    "The results are comparable to those for an LSTM model provided in Weston et al.:\n",
    "\"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\"\n",
    "http://arxiv.org/abs/1502.05698\n",
    "Task Number                  | FB LSTM Baseline | Keras QA\n",
    "---                          | ---              | ---\n",
    "QA1 - Single Supporting Fact | 50               | 100.0\n",
    "QA2 - Two Supporting Facts   | 20               | 50.0\n",
    "QA3 - Three Supporting Facts | 20               | 20.5\n",
    "QA4 - Two Arg. Relations     | 61               | 62.9\n",
    "QA5 - Three Arg. Relations   | 70               | 61.9\n",
    "QA6 - Yes/No Questions       | 48               | 50.7\n",
    "QA7 - Counting               | 49               | 78.9\n",
    "QA8 - Lists/Sets             | 45               | 77.2\n",
    "QA9 - Simple Negation        | 64               | 64.0\n",
    "QA10 - Indefinite Knowledge  | 44               | 47.7\n",
    "QA11 - Basic Coreference     | 72               | 74.9\n",
    "QA12 - Conjunction           | 74               | 76.4\n",
    "QA13 - Compound Coreference  | 94               | 94.4\n",
    "QA14 - Time Reasoning        | 27               | 34.8\n",
    "QA15 - Basic Deduction       | 21               | 32.4\n",
    "QA16 - Basic Induction       | 23               | 50.6\n",
    "QA17 - Positional Reasoning  | 51               | 49.1\n",
    "QA18 - Size Reasoning        | 52               | 90.8\n",
    "QA19 - Path Finding          | 8                | 9.0\n",
    "QA20 - Agent's Motivations   | 91               | 90.7\n",
    "For the resources related to the bAbI project, refer to:\n",
    "https://research.facebook.com/researchers/1543934539189348\n",
    "Notes:\n",
    "- With default word, sentence, and query vector sizes, the GRU model achieves:\n",
    "  - 100% test accuracy on QA1 in 20 epochs (2 seconds per epoch on CPU)\n",
    "  - 50% test accuracy on QA2 in 20 epochs (16 seconds per epoch on CPU)\n",
    "In comparison, the Facebook paper achieves 50% and 20% for the LSTM baseline.\n",
    "- The task does not traditionally parse the question separately. This likely\n",
    "improves accuracy and is a good example of merging two RNNs.\n",
    "- The word vector embeddings are not shared between the story and question RNNs.\n",
    "- See how the accuracy changes given 10,000 training samples (en-10k) instead\n",
    "of only 1000. 1000 was used in order to be comparable to the original paper.\n",
    "- Experiment with GRU, LSTM, and JZS1-3 as they give subtly different results.\n",
    "- The length and noise (i.e. 'useless' story components) impact the ability for\n",
    "LSTMs / GRUs to provide the correct answer. Given only the supporting facts,\n",
    "these RNNs can achieve 100% accuracy on many tasks. Memory networks and neural\n",
    "networks that use attentional processes can efficiently search through this\n",
    "noise to find the relevant statements, improving performance substantially.\n",
    "This becomes especially obvious on QA2 and QA3, both far longer than QA1.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Merge, Dropout, RepeatVector\n",
    "from keras.layers import recurrent\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbi tasks format\n",
    "    If only_supporting is true, only the sentences that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            substory = None\n",
    "            if only_supporting:\n",
    "                # Only select the relMAated substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''Given a file name, read the file, retrieve the stories, and then convert the sentences into a single story.\n",
    "    If max_length is supplied, any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    X = []\n",
    "    Xq = []\n",
    "    Y = []\n",
    "    for story, query, answer in data:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        y = np.zeros(len(word_idx) + 1)  # let's not forget that index 0 is reserved\n",
    "        y[word_idx[answer]] = 1\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "    return pad_sequences(X, maxlen=story_maxlen), pad_sequences(Xq, maxlen=query_maxlen), np.array(Y)\n",
    "\n",
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN, EMBED_HIDDEN_SIZE, SENT_HIDDEN_SIZE, QUERY_HIDDEN_SIZE))\n",
    "\n",
    "try:\n",
    "    path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n",
    "except:\n",
    "    print('Error downloading dataset, please download it manually:\\n'\n",
    "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n'\n",
    "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
    "    raise\n",
    "tar = tarfile.open(path)\n",
    "# Default QA1 with 1000 samples\n",
    "# challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt'\n",
    "challenge = 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt'\n",
    "# QA1 with 10,000 samples\n",
    "# challenge = 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt'\n",
    "# QA2 with 1000 samples\n",
    "# challenge = 'tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt'\n",
    "# QA2 with 10,000 samples\n",
    "# challenge = 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt'\n",
    "train = get_stories(tar.extractfile(challenge.format('train')))\n",
    "test = get_stories(tar.extractfile(challenge.format('test')))\n",
    "\n",
    "vocab = sorted(reduce(lambda x, y: x | y, (set(story + q + [answer]) for story, q, answer in train + test)))\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train + test)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train + test)))\n",
    "\n",
    "X, Xq, Y    = vectorize_stories(train, word_idx, story_maxlen, query_maxlen)\n",
    "tX, tXq, tY = vectorize_stories( test, word_idx, story_maxlen, query_maxlen)\n",
    "\n",
    "print('vocab = {}'.format(vocab))\n",
    "print('X.shape = {}'.format(X.shape))\n",
    "print('Xq.shape = {}'.format(Xq.shape))\n",
    "print('Y.shape = {}'.format(Y.shape))\n",
    "print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen))\n",
    "print('Build model...')\n",
    "\n",
    "qrnn = Sequential()\n",
    "qrnn.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE,\n",
    "                   input_length=query_maxlen))\n",
    "\n",
    "qrnn.add(Dropout(0.3))\n",
    "qrnn.add(RNN(EMBED_HIDDEN_SIZE, return_sequences=False))\n",
    "\n",
    "qrnn.add(RepeatVector(story_maxlen))\n",
    "\n",
    "sentrnn = Sequential()\n",
    "sentrnn.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE,\n",
    "                      input_length=story_maxlen))\n",
    "sentrnn.add(Dropout(0.3))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([sentrnn, qrnn], mode='sum'))\n",
    "model.add(RNN(EMBED_HIDDEN_SIZE, return_sequences=False))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Training')\n",
    "history = model.fit([X, Xq], Y, batch_size=BATCH_SIZE, nb_epoch=EPOCHS, validation_split=0.05)\n",
    "loss, acc = model.evaluate([tX, tXq], tY, batch_size=BATCH_SIZE)\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], acc*100))\n",
    "best = acc * 100\n",
    "# supportingFacts false\n",
    "# 10 k \n",
    "# 40 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 68, 50)        1100                                         \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 68, 50)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 4, 50)         1100                                         \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 4, 50)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 50)            20200                                        \n",
      "____________________________________________________________________________________________________\n",
      "repeatvector_1 (RepeatVector)    (None, 68, 50)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (None, 50)            20200       merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 50)            0           lstm_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 22)            1122        dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 43,722\n",
      "Trainable params: 43,722\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "{'verbose': 1, 'nb_epoch': 40, 'batch_size': 32, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc'], 'nb_sample': 9500, 'do_validation': True}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAekAAAEZCAYAAACzXN2OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npxTSIiIicUohLSIiEqf+PxxsvibjGWx6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3c643ddc10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEZCAYAAAC0HgObAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3c63ee6d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as p\n",
    "%matplotlib inline\n",
    "model.summary()\n",
    "print (history.params)\n",
    "\n",
    "x = history.epoch\n",
    "y_acc = history.history[\"acc\"]\n",
    "y_accVal = history.history['val_acc']\n",
    "\n",
    "p.plot(x, y_acc, label='Train')\n",
    "p.plot(x, y_accVal, label='Test')\n",
    "p.legend(bbox_to_anchor=(0, 1),\n",
    "           bbox_transform=p.gcf().transFigure)\n",
    "title = 'Model base' + '\\nAccuracy' + 'Standard'\n",
    "p.title(title)\n",
    "\n",
    "p.show()\n",
    "\n",
    "y_loss = history.history[\"loss\"]\n",
    "y_lossVal = history.history['val_loss']\n",
    "p.plot(x, y_loss, label='Train')\n",
    "p.plot(x, y_lossVal, label='Test')\n",
    "p.legend(bbox_to_anchor=(1, 1),\n",
    "           bbox_transform=p.gcf().transFigure)\n",
    "title1 = 'Model base' + '\\nLoss' + 'Standard'\n",
    "p.title(title1)\n",
    "\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/40\n",
      "950/950 [==============================] - 5s - loss: 2.5647 - acc: 0.1716 - val_loss: 2.0381 - val_acc: 0.3600\n",
      "Epoch 2/40\n",
      "950/950 [==============================] - 5s - loss: 1.9362 - acc: 0.2232 - val_loss: 1.7080 - val_acc: 0.5000\n",
      "Epoch 3/40\n",
      "950/950 [==============================] - 5s - loss: 1.7239 - acc: 0.3147 - val_loss: 1.6268 - val_acc: 0.4200\n",
      "Epoch 4/40\n",
      "950/950 [==============================] - 5s - loss: 1.6080 - acc: 0.4042 - val_loss: 1.4475 - val_acc: 0.5200\n",
      "Epoch 5/40\n",
      "950/950 [==============================] - 5s - loss: 1.4499 - acc: 0.4853 - val_loss: 1.4716 - val_acc: 0.4600\n",
      "Epoch 6/40\n",
      "950/950 [==============================] - 5s - loss: 1.4183 - acc: 0.5053 - val_loss: 1.3427 - val_acc: 0.5000\n",
      "Epoch 7/40\n",
      "950/950 [==============================] - 5s - loss: 1.3357 - acc: 0.5389 - val_loss: 1.3182 - val_acc: 0.5000\n",
      "Epoch 8/40\n",
      "950/950 [==============================] - 5s - loss: 1.2522 - acc: 0.5495 - val_loss: 1.2987 - val_acc: 0.5200\n",
      "Epoch 9/40\n",
      "950/950 [==============================] - 5s - loss: 1.2704 - acc: 0.5442 - val_loss: 1.2778 - val_acc: 0.5400\n",
      "Epoch 10/40\n",
      "950/950 [==============================] - 5s - loss: 1.2170 - acc: 0.5684 - val_loss: 1.2586 - val_acc: 0.5000\n",
      "Epoch 11/40\n",
      "950/950 [==============================] - 5s - loss: 1.2201 - acc: 0.5579 - val_loss: 1.2179 - val_acc: 0.4400\n",
      "Epoch 12/40\n",
      "950/950 [==============================] - 5s - loss: 1.1687 - acc: 0.5684 - val_loss: 1.2202 - val_acc: 0.5000\n",
      "Epoch 13/40\n",
      "950/950 [==============================] - 5s - loss: 1.1338 - acc: 0.5884 - val_loss: 1.2794 - val_acc: 0.5200\n",
      "Epoch 14/40\n",
      "950/950 [==============================] - 5s - loss: 1.1672 - acc: 0.5326 - val_loss: 1.2650 - val_acc: 0.5000\n",
      "Epoch 15/40\n",
      "950/950 [==============================] - 5s - loss: 1.1126 - acc: 0.5905 - val_loss: 1.3007 - val_acc: 0.5200\n",
      "Epoch 16/40\n",
      "950/950 [==============================] - 5s - loss: 1.1010 - acc: 0.5726 - val_loss: 1.0936 - val_acc: 0.4800\n",
      "Epoch 17/40\n",
      "950/950 [==============================] - 5s - loss: 1.1169 - acc: 0.5789 - val_loss: 1.3226 - val_acc: 0.5000\n",
      "Epoch 18/40\n",
      "950/950 [==============================] - 5s - loss: 1.0971 - acc: 0.5674 - val_loss: 1.2023 - val_acc: 0.4200\n",
      "Epoch 19/40\n",
      "950/950 [==============================] - 5s - loss: 1.0677 - acc: 0.5874 - val_loss: 1.1242 - val_acc: 0.5400\n",
      "Epoch 20/40\n",
      "950/950 [==============================] - 5s - loss: 1.0679 - acc: 0.5779 - val_loss: 1.3538 - val_acc: 0.4600\n",
      "Epoch 21/40\n",
      "950/950 [==============================] - 5s - loss: 1.0605 - acc: 0.5968 - val_loss: 1.3546 - val_acc: 0.4600\n",
      "Epoch 22/40\n",
      "950/950 [==============================] - 5s - loss: 1.0883 - acc: 0.5768 - val_loss: 1.1856 - val_acc: 0.5000\n",
      "Epoch 23/40\n",
      "950/950 [==============================] - 5s - loss: 1.0597 - acc: 0.5853 - val_loss: 1.2155 - val_acc: 0.5200\n",
      "Epoch 24/40\n",
      "950/950 [==============================] - 5s - loss: 1.0289 - acc: 0.5958 - val_loss: 1.2334 - val_acc: 0.5400\n",
      "Epoch 25/40\n",
      "950/950 [==============================] - 5s - loss: 1.0994 - acc: 0.5632 - val_loss: 1.2433 - val_acc: 0.5000\n",
      "Epoch 26/40\n",
      "950/950 [==============================] - 5s - loss: 1.0653 - acc: 0.5726 - val_loss: 1.2301 - val_acc: 0.4800\n",
      "Epoch 27/40\n",
      "950/950 [==============================] - 5s - loss: 1.0386 - acc: 0.5895 - val_loss: 1.2039 - val_acc: 0.5000\n",
      "Epoch 28/40\n",
      "950/950 [==============================] - 5s - loss: 1.0081 - acc: 0.5968 - val_loss: 1.2104 - val_acc: 0.5000\n",
      "Epoch 29/40\n",
      "950/950 [==============================] - 5s - loss: 1.0089 - acc: 0.5979 - val_loss: 1.1391 - val_acc: 0.4800\n",
      "Epoch 30/40\n",
      "950/950 [==============================] - 5s - loss: 0.9956 - acc: 0.6126 - val_loss: 1.1649 - val_acc: 0.5800\n",
      "Epoch 31/40\n",
      "950/950 [==============================] - 5s - loss: 0.9882 - acc: 0.6189 - val_loss: 1.0994 - val_acc: 0.5200\n",
      "Epoch 32/40\n",
      "950/950 [==============================] - 5s - loss: 1.0008 - acc: 0.6063 - val_loss: 1.1357 - val_acc: 0.5400\n",
      "Epoch 33/40\n",
      "950/950 [==============================] - 5s - loss: 0.9854 - acc: 0.6084 - val_loss: 1.2350 - val_acc: 0.5200\n",
      "Epoch 34/40\n",
      "950/950 [==============================] - 5s - loss: 0.9856 - acc: 0.6095 - val_loss: 1.1956 - val_acc: 0.4800\n",
      "Epoch 35/40\n",
      "950/950 [==============================] - 5s - loss: 0.9431 - acc: 0.6179 - val_loss: 1.1116 - val_acc: 0.5200\n",
      "Epoch 36/40\n",
      "950/950 [==============================] - 5s - loss: 0.9325 - acc: 0.6400 - val_loss: 1.1342 - val_acc: 0.5200\n",
      "Epoch 37/40\n",
      "950/950 [==============================] - 5s - loss: 0.9482 - acc: 0.6400 - val_loss: 1.2663 - val_acc: 0.4800\n",
      "Epoch 38/40\n",
      "950/950 [==============================] - 5s - loss: 0.9251 - acc: 0.6179 - val_loss: 1.1528 - val_acc: 0.5000\n",
      "Epoch 39/40\n",
      "950/950 [==============================] - 5s - loss: 0.9076 - acc: 0.6463 - val_loss: 1.3702 - val_acc: 0.4800\n",
      "Epoch 40/40\n",
      "950/950 [==============================] - 5s - loss: 0.9071 - acc: 0.6484 - val_loss: 1.1671 - val_acc: 0.5000\n",
      "1000/1000 [==============================] - 1s     \n",
      "Test loss / test accuracy = 1.2958 / 0.4980\n",
      "acc: 49.80%\n",
      " Not the best !\n"
     ]
    }
   ],
   "source": [
    "qrnn2 = Sequential()\n",
    "qrnn2.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE,\n",
    "                   input_length=query_maxlen))\n",
    "\n",
    "qrnn2.add(Dropout(0.3))\n",
    "qrnn2.add(RNN(EMBED_HIDDEN_SIZE, return_sequences=False))\n",
    "qrnn2.add(Dense(EMBED_HIDDEN_SIZE, activation='relu'))\n",
    "qrnn2.add(RepeatVector(story_maxlen))\n",
    "\n",
    "sentrnn2 = Sequential()\n",
    "sentrnn2.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE,\n",
    "                      input_length=story_maxlen))\n",
    "sentrnn2.add(Dropout(0.3))\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Merge([sentrnn, qrnn], mode='sum'))\n",
    "model2.add(RNN(EMBED_HIDDEN_SIZE, return_sequences=False))\n",
    "model2.add(Dropout(0.3))\n",
    "model2.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Training')\n",
    "history2 = model2.fit([X, Xq], Y, batch_size=BATCH_SIZE, nb_epoch=EPOCHS, validation_split=0.05)\n",
    "loss, acc = model2.evaluate([tX, tXq], tY, batch_size=BATCH_SIZE)\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))\n",
    "print(\"%s: %.2f%%\" % (model2.metrics_names[1], acc*100))\n",
    "if (acc*100) < best:\n",
    "    print(\" Not the best !\")\n",
    "else:\n",
    "    print(\" We found a winner !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 66, 50)        1100                                         \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 66, 50)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 4, 50)         1100                                         \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 4, 50)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 50)            20200                                        \n",
      "____________________________________________________________________________________________________\n",
      "repeatvector_1 (RepeatVector)    (None, 66, 50)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 50)            20200       merge_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 50)            0           lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 22)            1122        dropout_6[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 43,722\n",
      "Trainable params: 43,722\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "{'verbose': 1, 'nb_epoch': 40, 'batch_size': 32, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc'], 'nb_sample': 950, 'do_validation': True}\n"
     ]
    }
   ],
   "source": [
    "model2.summary()\n",
    "print (history2.params)\n",
    "\n",
    "y_acc2 = history2.history[\"acc\"]\n",
    "y_accVal2 = history2.history['val_acc']\n",
    "\n",
    "p.plot(x, y_acc2, label='Train')\n",
    "p.plot(x, y_accVal2, label='Test')\n",
    "p.legend(bbox_to_anchor=(1, 1),\n",
    "           bbox_transform=p.gcf().transFigure)\n",
    "title = 'Model LSTM + Dense relu' + '\\nAccuracy' + '\\nStandard'\n",
    "p.title(title)\n",
    "p.title(title1)\n",
    "p.show()\n",
    "\n",
    "y_loss2 = history2.history[\"loss\"]\n",
    "y_lossVal2 = history2.history['val_loss']\n",
    "p.plot(x, y_loss2, label='Train')\n",
    "p.plot(x, y_lossVal2, label='Test')\n",
    "p.legend(bbox_to_anchor=(1, 1),\n",
    "           bbox_transform=p.gcf().transFigure)\n",
    "title1 = 'Model LSTM + Dense relu' + '\\nLoss' + '\\nStandard'\n",
    "p.title(title)\n",
    "\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print([key for key in word_idx.keys() if word_idx[key] == 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/50\n",
      "950/950 [==============================] - 5s - loss: 2.6023 - acc: 0.1547 - val_loss: 1.8717 - val_acc: 0.2800\n",
      "Epoch 2/50\n",
      "950/950 [==============================] - 5s - loss: 1.8166 - acc: 0.2516 - val_loss: 1.6293 - val_acc: 0.5000\n",
      "Epoch 3/50\n",
      "950/950 [==============================] - 6s - loss: 1.6664 - acc: 0.3537 - val_loss: 1.5960 - val_acc: 0.5000\n",
      "Epoch 4/50\n",
      "950/950 [==============================] - 5s - loss: 1.5646 - acc: 0.4263 - val_loss: 1.4819 - val_acc: 0.5000\n",
      "Epoch 5/50\n",
      "950/950 [==============================] - 5s - loss: 1.5077 - acc: 0.4695 - val_loss: 1.4312 - val_acc: 0.5000\n",
      "Epoch 6/50\n",
      "950/950 [==============================] - 5s - loss: 1.4714 - acc: 0.4768 - val_loss: 1.3753 - val_acc: 0.5400\n",
      "Epoch 7/50\n",
      "950/950 [==============================] - 5s - loss: 1.4089 - acc: 0.5074 - val_loss: 1.4167 - val_acc: 0.5000\n",
      "Epoch 8/50\n",
      "950/950 [==============================] - 5s - loss: 1.3740 - acc: 0.5084 - val_loss: 1.3616 - val_acc: 0.5400\n",
      "Epoch 9/50\n",
      "950/950 [==============================] - 5s - loss: 1.3438 - acc: 0.5274 - val_loss: 1.5100 - val_acc: 0.4600\n",
      "Epoch 10/50\n",
      "950/950 [==============================] - 5s - loss: 1.3138 - acc: 0.5200 - val_loss: 1.3827 - val_acc: 0.4800\n",
      "Epoch 11/50\n",
      "950/950 [==============================] - 5s - loss: 1.2957 - acc: 0.5053 - val_loss: 1.3657 - val_acc: 0.5000\n",
      "Epoch 12/50\n",
      "950/950 [==============================] - 5s - loss: 1.2670 - acc: 0.5158 - val_loss: 1.4552 - val_acc: 0.4800\n",
      "Epoch 13/50\n",
      "950/950 [==============================] - 5s - loss: 1.3076 - acc: 0.5158 - val_loss: 1.4067 - val_acc: 0.5200\n",
      "Epoch 14/50\n",
      "950/950 [==============================] - 5s - loss: 1.2454 - acc: 0.5337 - val_loss: 1.3867 - val_acc: 0.4600\n",
      "Epoch 15/50\n",
      "950/950 [==============================] - 5s - loss: 1.2508 - acc: 0.5337 - val_loss: 1.3481 - val_acc: 0.5200\n",
      "Epoch 16/50\n",
      "950/950 [==============================] - 5s - loss: 1.2067 - acc: 0.5442 - val_loss: 1.2902 - val_acc: 0.5200\n",
      "Epoch 17/50\n",
      "950/950 [==============================] - 5s - loss: 1.2173 - acc: 0.5295 - val_loss: 1.3228 - val_acc: 0.5200\n",
      "Epoch 18/50\n",
      "950/950 [==============================] - 5s - loss: 1.2124 - acc: 0.5474 - val_loss: 1.3250 - val_acc: 0.5000\n",
      "Epoch 19/50\n",
      "950/950 [==============================] - 5s - loss: 1.1813 - acc: 0.5253 - val_loss: 1.2908 - val_acc: 0.5600\n",
      "Epoch 20/50\n",
      "950/950 [==============================] - 5s - loss: 1.1662 - acc: 0.5579 - val_loss: 1.2807 - val_acc: 0.5400\n",
      "Epoch 21/50\n",
      "950/950 [==============================] - 5s - loss: 1.2138 - acc: 0.5463 - val_loss: 1.3407 - val_acc: 0.4800\n",
      "Epoch 22/50\n",
      "950/950 [==============================] - 5s - loss: 1.1213 - acc: 0.5853 - val_loss: 1.2756 - val_acc: 0.5200\n",
      "Epoch 23/50\n",
      "950/950 [==============================] - 5s - loss: 1.1416 - acc: 0.5726 - val_loss: 1.2955 - val_acc: 0.5400\n",
      "Epoch 24/50\n",
      "950/950 [==============================] - 5s - loss: 1.1426 - acc: 0.5526 - val_loss: 1.3385 - val_acc: 0.5000\n",
      "Epoch 25/50\n",
      "950/950 [==============================] - 5s - loss: 1.1342 - acc: 0.5568 - val_loss: 1.2424 - val_acc: 0.5200\n",
      "Epoch 26/50\n",
      "950/950 [==============================] - 6s - loss: 1.1273 - acc: 0.5558 - val_loss: 1.3105 - val_acc: 0.5000\n",
      "Epoch 27/50\n",
      "950/950 [==============================] - 6s - loss: 1.0896 - acc: 0.5779 - val_loss: 1.2269 - val_acc: 0.5200\n",
      "Epoch 28/50\n",
      "950/950 [==============================] - 6s - loss: 1.0961 - acc: 0.5600 - val_loss: 1.2040 - val_acc: 0.5000\n",
      "Epoch 29/50\n",
      "950/950 [==============================] - 6s - loss: 1.0838 - acc: 0.5674 - val_loss: 1.3253 - val_acc: 0.5000\n",
      "Epoch 30/50\n",
      "950/950 [==============================] - 6s - loss: 1.1002 - acc: 0.5621 - val_loss: 1.3594 - val_acc: 0.5800\n",
      "Epoch 31/50\n",
      "950/950 [==============================] - 6s - loss: 1.0671 - acc: 0.5695 - val_loss: 1.3646 - val_acc: 0.4600\n",
      "Epoch 32/50\n",
      "950/950 [==============================] - 5s - loss: 1.0820 - acc: 0.5695 - val_loss: 1.2630 - val_acc: 0.5200\n",
      "Epoch 33/50\n",
      "950/950 [==============================] - 6s - loss: 1.0426 - acc: 0.5789 - val_loss: 1.2028 - val_acc: 0.5400\n",
      "Epoch 34/50\n",
      "950/950 [==============================] - 6s - loss: 1.0540 - acc: 0.5863 - val_loss: 1.2999 - val_acc: 0.5200\n",
      "Epoch 35/50\n",
      "950/950 [==============================] - 5s - loss: 1.0599 - acc: 0.5779 - val_loss: 1.2735 - val_acc: 0.5600\n",
      "Epoch 36/50\n",
      "950/950 [==============================] - 5s - loss: 1.0362 - acc: 0.5937 - val_loss: 1.2660 - val_acc: 0.5400\n",
      "Epoch 37/50\n",
      "950/950 [==============================] - 6s - loss: 1.0583 - acc: 0.5653 - val_loss: 1.3276 - val_acc: 0.5000\n",
      "Epoch 38/50\n",
      "950/950 [==============================] - 6s - loss: 1.0758 - acc: 0.5589 - val_loss: 1.2972 - val_acc: 0.5400\n",
      "Epoch 39/50\n",
      "950/950 [==============================] - 5s - loss: 1.0483 - acc: 0.5621 - val_loss: 1.2951 - val_acc: 0.5400\n",
      "Epoch 40/50\n",
      "950/950 [==============================] - 5s - loss: 1.0544 - acc: 0.5589 - val_loss: 1.2893 - val_acc: 0.5600\n",
      "Epoch 41/50\n",
      "950/950 [==============================] - 6s - loss: 1.0011 - acc: 0.6147 - val_loss: 1.3289 - val_acc: 0.5000\n",
      "Epoch 42/50\n",
      "950/950 [==============================] - 6s - loss: 1.0165 - acc: 0.5747 - val_loss: 1.2532 - val_acc: 0.5600\n",
      "Epoch 43/50\n",
      "950/950 [==============================] - 6s - loss: 1.0164 - acc: 0.5789 - val_loss: 1.2797 - val_acc: 0.5600\n",
      "Epoch 44/50\n",
      "950/950 [==============================] - 6s - loss: 0.9853 - acc: 0.5800 - val_loss: 1.3678 - val_acc: 0.5400\n",
      "Epoch 45/50\n",
      "950/950 [==============================] - 6s - loss: 0.9924 - acc: 0.5968 - val_loss: 1.4011 - val_acc: 0.5200\n",
      "Epoch 46/50\n",
      "950/950 [==============================] - 6s - loss: 0.9905 - acc: 0.5905 - val_loss: 1.3073 - val_acc: 0.5200\n",
      "Epoch 47/50\n",
      "950/950 [==============================] - 5s - loss: 0.9915 - acc: 0.5979 - val_loss: 1.2721 - val_acc: 0.5200\n",
      "Epoch 48/50\n",
      "950/950 [==============================] - 5s - loss: 0.9958 - acc: 0.5895 - val_loss: 1.2915 - val_acc: 0.5200\n",
      "Epoch 49/50\n",
      "950/950 [==============================] - 6s - loss: 0.9847 - acc: 0.6137 - val_loss: 1.2510 - val_acc: 0.5400\n",
      "Epoch 50/50\n",
      "950/950 [==============================] - 5s - loss: 0.9836 - acc: 0.5884 - val_loss: 1.3064 - val_acc: 0.5400\n",
      "1000/1000 [==============================] - 1s     \n",
      "Test loss / test accuracy = 1.2556 / 0.5120\n",
      "acc: 51.20%\n"
     ]
    }
   ],
   "source": [
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 50\n",
    "\n",
    "\n",
    "qrnn3 = Sequential()\n",
    "qrnn3.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE,\n",
    "                   input_length=query_maxlen))\n",
    "\n",
    "qrnn3.add(Dropout(0.3))\n",
    "qrnn3.add(RNN(EMBED_HIDDEN_SIZE, return_sequences=False))\n",
    "qrnn3.add(RepeatVector(story_maxlen))\n",
    "\n",
    "sentrnn3 = Sequential()\n",
    "sentrnn3.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE,\n",
    "                      input_length=story_maxlen))\n",
    "sentrnn3.add(Dropout(0.3))\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Merge([sentrnn, qrnn], mode='sum'))\n",
    "model3.add(RNN(EMBED_HIDDEN_SIZE, return_sequences=False))\n",
    "model3.add(Dropout(0.3))\n",
    "model3.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Training')\n",
    "history3 = model3.fit([X, Xq], Y, batch_size=BATCH_SIZE, nb_epoch=EPOCHS, validation_split=0.05)\n",
    "loss, acc = model3.evaluate([tX, tXq], tY, batch_size=BATCH_SIZE)\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))\n",
    "print(\"%s: %.2f%%\" % (model3.metrics_names[1], acc*100))\n",
    "\n",
    "if (acc*100) < best:\n",
    "    print(\" Not the best !\")\n",
    "else:\n",
    "    print(\" We found a winner !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 66, 50)        1100                                         \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 66, 50)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 66, 50)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 4, 50)         1100                                         \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 4, 50)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 50)            20200                                        \n",
      "____________________________________________________________________________________________________\n",
      "repeatvector_1 (RepeatVector)    (None, 66, 50)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                    (None, 100)           60400       merge_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 100)           0           lstm_6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 22)            2222        dropout_9[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 85,022\n",
      "Trainable params: 85,022\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "{'verbose': 1, 'nb_epoch': 40, 'batch_size': 32, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc'], 'nb_sample': 950, 'do_validation': True}\n"
     ]
    }
   ],
   "source": [
    "model3.summary()\n",
    "print (history3.params)\n",
    "\n",
    "y_acc3 = history3.history[\"acc\"]\n",
    "y_accVal3 = history3.history['val_acc']\n",
    "\n",
    "p.plot(x,y_acc3, label='Train')\n",
    "p.plot(x,y_accVal3, label='Test')\n",
    "p.legend(bbox_to_anchor=(1, -1),\n",
    "           bbox_transform=p.gcf().transFigure)\n",
    "title = 'Model LSTM + Dense relu' + '\\nAccuracy' + '\\nEMBED_HIDDEN_SIZE = 100'\n",
    "p.title(title)\n",
    "\n",
    "p.show()\n",
    "\n",
    "y_loss3 = history3.history[\"loss\"]\n",
    "y_lossVal3 = history3.history['val_loss']\n",
    "p.plot(x, y_loss3, label='Train')\n",
    "p.plot(x, y_lossVal3, label='Test')\n",
    "p.legend(bbox_to_anchor=(1, -1),\n",
    "           bbox_transform=p.gcf().transFigure)\n",
    "title = 'Model LSTM + Dense relu' + '\\nLoss'  + '\\nEMBED_HIDDEN_SIZE = 100'\n",
    "p.title(title)\n",
    "\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the supporting ! RN which  have to learn where is the answer\n",
    "#\n",
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 50\n",
    "\n",
    "\n",
    "qrnn4 = Sequential()\n",
    "qrnn4.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE,\n",
    "                   input_length=query_maxlen))\n",
    "\n",
    "qrnn4.add(Dropout(0.3))\n",
    "qrnn4.add(RNN(EMBED_HIDDEN_SIZE, return_sequences=False))\n",
    "qrnn4.add(RepeatVector(story_maxlen))\n",
    "\n",
    "sentrnn4 = Sequential()\n",
    "sentrnn4.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE,\n",
    "                      input_length=story_maxlen))\n",
    "sentrnn4.add(Dropout(0.3))\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Merge([sentrnn, qrnn], mode='sum'))\n",
    "model4.add(RNN(EMBED_HIDDEN_SIZE, return_sequences=False))\n",
    "model4.add(Dropout(0.3))\n",
    "model4.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# For a multi-class classification problem\n",
    "model4.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Training')\n",
    "history4 = model4.fit([X, Xq], Y, batch_size=BATCH_SIZE, nb_epoch=EPOCHS, validation_split=0.05)\n",
    "loss, acc = model4.evaluate([tX, tXq], tY, batch_size=BATCH_SIZE)\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))\n",
    "print(\"%s: %.2f%%\" % (model4.metrics_names[1], acc*100))\n",
    "\n",
    "if (acc*100) < best:\n",
    "    print(\" Not the best !\")\n",
    "else:\n",
    "    print(\" We found a winner !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model4.summary()\n",
    "print (history4.params)\n",
    "\n",
    "y_acc4 = history4.history[\"acc\"]\n",
    "y_accVal4 = history4.history['val_acc']\n",
    "\n",
    "p.plot(x,y_acc4, label='Train')\n",
    "p.plot(x,y_accVal4, label='Test')\n",
    "p.legend(bbox_to_anchor=(1, -1),\n",
    "           bbox_transform=p.gcf().transFigure)\n",
    "title = 'Model LSTM + Dense relu' + '\\nAccuracy' + '\\nEMBED_HIDDEN_SIZE = 100'\n",
    "p.title(title)\n",
    "\n",
    "p.show()\n",
    "\n",
    "y_loss4 = history4.history[\"loss\"]\n",
    "y_lossVal4 = history4.history['val_loss']\n",
    "p.plot(x, y_loss4, label='Train')\n",
    "p.plot(x, y_lossVal4, label='Test')\n",
    "p.legend(bbox_to_anchor=(1, -1),\n",
    "           bbox_transform=p.gcf().transFigure)\n",
    "title = 'Model LSTM + Dense relu' + '\\nLoss'  + '\\nEMBED_HIDDEN_SIZE = 100'\n",
    "p.title(title)\n",
    "\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
